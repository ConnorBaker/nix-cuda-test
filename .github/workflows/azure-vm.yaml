name: Azure VM

defaults:
  run:
    shell: bash

on:
  workflow_dispatch:
    inputs:
      # TODO: Implement the ability to stop it
      action:
        description: Action
        required: true
        type: choice
        options:
          - start
          - terminate
      size:
        description: Instance
        required: true
        type: choice
        # All numbers are for spot instances in East US as of 2023-06-03.
        # Standard_HB120rs_v3:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/hbv3-series
        # - 120 AMD EPYCâ„¢ 7V73X (Milan-X) CPU cores
        # - 448 GB of RAM
        # - 2 * 960 GB SSD
        # - $0.37/hour
        # Standard_NC8as_T4_v3:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/nct4-v3-series
        # - 1x NVIDIA Tesla T4 GPU with 16 GB of RAM
        # - 8 AMD EPYC 7V12 (Rome) CPU cores
        # - 56 GB of RAM
        # - 1 * 360 GB SSD
        # - $0.14/hour
        # Standard_NV36ads_A10_v5:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series
        # - 1x NVIDIA A10 GPU with 24 GB of RAM
        # - 36 AMD EPYC 74F3V(Milan) CPU cores
        # - 440 GB of RAM
        # - 1 * 1440 GB SSD
        # - $1.28/hour
        options:
          - Standard_HB120rs_v3
          # - Standard_NC8as_T4_v3
          # - Standard_NV36ads_A10_v5

jobs:
  get-vhd-name:
    name: Create a reference to the VHD name for later use
    runs-on: ubuntu-latest
    outputs:
      vhd-name: ${{ steps.get-vhd-name.outputs.vhd-name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Nix
        uses: ./.github/actions/nix-setup
        with:
          cant-cache-me-nix-secret-signing-key: ${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}
          cloudflare-r2-access-key-id: ${{ secrets.CLOUDFLARE_R2_ACCESS_KEY_ID }}
          cloudflare-r2-secret-access-key: ${{ secrets.CLOUDFLARE_R2_SECRET_ACCESS_KEY }}

      - name: Get the VHD name
        id: get-vhd-name
        run: |
          nix eval --raw .#nixosConfigurations.azure.outPath \
            | sed 's|/nix/store/|vhd-name=|' \
            | tee -a "$GITHUB_OUTPUT"

  ensure-nixos-azure-vhd-blob-exists:
    name: Ensure NixOS Azure VHD blob exists
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'start' }}
    needs: get-vhd-name
    outputs:
      vhd-name: ${{ needs.get-vhd-name.outputs.vhd-name }}
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_AUTH_MODE: login
      VHD_NAME: ${{ needs.get-vhd-name.outputs.vhd-name }}
    steps:
      - name: Log in to Azure
        uses: Azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      - name: Check if VHD exists in blob storage
        id: check-vhd-blob-exists
        run: |
          az storage blob exists \
              --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
              --name "${{ env.VHD_NAME }}.vhd" \
            | jq -cr '"vhd-blob-exists=\(.exists)"' \
            | tee -a "$GITHUB_OUTPUT"

      - name: Restore the VHD from GitHub Actions' cache
        env:
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.VHD_BLOB_EXISTS != 'true' }}
        id: restore-vhd
        uses: actions/cache/restore@v3
        with:
          key: ${{ env.VHD_NAME }}
          path: ./result/disk.vhd

      - name: Checkout
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        uses: actions/checkout@v3

      - name: Set up Nix
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        uses: ./.github/actions/nix-setup
        with:
          cant-cache-me-nix-secret-signing-key: ${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}
          cloudflare-r2-access-key-id: ${{ secrets.CLOUDFLARE_R2_ACCESS_KEY_ID }}
          cloudflare-r2-secret-access-key: ${{ secrets.CLOUDFLARE_R2_SECRET_ACCESS_KEY }}

      - name: Download the NAR for the VHD
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        run: |
          nix store dump-path -v \
              --store https://cantcache.me \
              "/nix/store/${{ env.VHD_NAME }}" \
            > "./${{ env.VHD_NAME }}.nar"

      - name: Extract the VHD from the NAR
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        run: |
          mkdir "./result"
          nix nar cat -v "./${{ env.VHD_NAME }}.nar" /disk.vhd \
            > "./result/disk.vhd"

      - name: Copy the VHD to GitHub Actions' cache
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        uses: actions/cache/save@v3
        with:
          key: ${{ env.VHD_NAME }}
          path: ./result/disk.vhd

      - name: Upload the VHD to an Azure blob
        env:
          CACHE_HIT: ${{ steps.restore-vhd.outputs.cache-hit }}
          VHD_BLOB_EXISTS: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists }}
        if: ${{ env.CACHE_HIT != 'true' && env.VHD_BLOB_EXISTS != 'true' }}
        run: |
          az storage blob upload \
            --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
            --file "./result/disk.vhd" \
            --max-connections 8 \
            --name "${{ env.VHD_NAME }}.vhd" \
            --type page

  # We use the store entry as the name of the VM.
  # This lets us check if a VM running on the same image already exists.
  ensure-nixos-azure-vm-is-running:
    name: Ensure NixOS Azure VM is running
    needs: ensure-nixos-azure-vhd-blob-exists
    if: ${{ inputs.action == 'start' }}
    runs-on: ubuntu-latest
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_AUTH_MODE: login
      VHD_NAME: ${{ needs.ensure-nixos-azure-vhd-blob-exists.outputs.vhd-name }}
    outputs:
      vhd-name: ${{ needs.ensure-nixos-azure-vhd-blob-exists.outputs.vhd-name }}
      public-ip-address: ${{ steps.get-public-ip-address.outputs.public-ip-address }}
    steps:
      - name: Log in to Azure
        uses: Azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      # TODO(@connorbaker): Allow multiple VMs to be created.
      - name: Check if VMs are running
        id: check-vm-running
        run: |
          VMS=$(az vm list --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}")
          echo "$VMS" | jq

          MATCHING_VMS=$(echo "$VMS" | jq -cr --arg name "${{ env.VHD_NAME }}" 'map(select(.name == $name))')
          echo "$MATCHING_VMS" | jq

          NUM_MATCHING_VMS=$(echo "$MATCHING_VMS" | jq -cr length)
          echo "Number of matching VMs: $NUM_MATCHING_VMS"
          case $NUM_MATCHING_VMS in
            0)
              echo "vm-running=false" >> "$GITHUB_OUTPUT"
              ;;
            1)
              echo "vm-running=true" >> "$GITHUB_OUTPUT"
              ;;
            *)
              echo "Multiple VMs with the same name found" >&2
              echo "$MATCHING_VMS" >&2
              exit 1
              ;;
          esac

      - name: Get the VHD blob URL
        id: get-vhd-blob-url
        env:
          VM_RUNNING: ${{ steps.check-vm-running.outputs.vm-running }}
        if: ${{ env.VM_RUNNING != 'true' }}
        run: |
          az storage blob url \
              --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
              --name "${{ env.VHD_NAME }}.vhd" \
            | jq -cr '"vhd-blob-url=\(.)"' \
            | tee -a "$GITHUB_OUTPUT"

      # TODO(@connorbaker): Allow multiple VMs to be created.
      - name: Create the VM
        env:
          VHD_BLOB_URL: ${{ steps.get-vhd-blob-url.outputs.vhd-blob-url }}
          VM_RUNNING: ${{ steps.check-vm-running.outputs.vm-running }}
        if: ${{ env.VM_RUNNING != 'true' }}
        run: |
          az vm create \
            --attach-os-disk "${{ env.VHD_BLOB_URL }}" \
            --enable-hibernation false \
            --enable-hotpatching false \
            --eviction-policy Delete \
            --location eastus \
            --max-price 1.0 \
            --name "${{ env.VHD_NAME }}" \
            --nic-delete-option Detach \
            --os-type linux \
            --priority Spot \
            --public-ip-address-allocation dynamic \
            --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}" \
            --size "${{ inputs.size }}" \
            --use-unmanaged-disk

      - name: Wait for the VM to be created
        env:
          VM_RUNNING: ${{ steps.check-vm-running.outputs.vm-running }}
        if: ${{ env.VM_RUNNING != 'true' }}
        run: |
          az vm wait \
            --created \
            --name "${{ env.VHD_NAME }}" \
            --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}"

      - name: Get the public IP address of the VM
        id: get-public-ip-address
        run: |
          az vm show \
              --name "${{ env.VHD_NAME }}" \
              --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}" \
              --show-details \
            | jq -cr '"public-ip-address=\(.publicIps)"' \
            | tee -a "$GITHUB_OUTPUT"

  # TODO(@connorbaker): Refactor to allow setting up multiple VMs.
  ensure-nixos-azure-vm-hercules-ci-configured:
    name: Ensure NixOS Azure VM has Hercules CI configured
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'start' }}
    needs: ensure-nixos-azure-vm-is-running
    env:
      AZURE_VM_IP: ${{ needs.ensure-nixos-azure-vm-is-running.outputs.public-ip-address }}
    steps:
      - name: Set up the GitHub runner SSH keys
        run: |
          mkdir -p "$HOME/.ssh"
          chmod 700 "$HOME/.ssh"
          echo "${{ secrets.RUNNER_SSH_PRIVATE_KEY }}" > "$HOME/.ssh/ed25519"
          chmod 600 "$HOME/.ssh/ed25519"
          echo "${{ secrets.RUNNER_SSH_PUBLIC_KEY }}" > "$HOME/.ssh/ed25519.pub"
          chmod 600 "$HOME/.ssh/ed25519.pub"
          cat >> "$HOME/.ssh/config" <<EOF
          Host vm
            HostName ${{ env.AZURE_VM_IP }}
            User runner
            IdentityFile $HOME/.ssh/ed25519
            StrictHostKeyChecking no
          EOF
          chmod 600 "$HOME/.ssh/config"

      - name: Check if the agent is active (and so configured) with a timeout of 10 minutes
        id: hercules-ci-agent-active
        run: |
          ssh -o ConnectTimeout=600 vm \
              "sudo systemctl is-active --quiet hercules-ci-agent.service" \
            && echo "active=true" \
            || echo "active=false" \
            | tee -a "$GITHUB_OUTPUT"

      - name: Stop the agent
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: ssh vm "sudo systemctl stop hercules-ci-agent.service"

      - name: Create the agent secrets directory
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: ssh vm "sudo mkdir -p /var/lib/hercules-ci-agent/secrets"

      - name: Create the agent binary-caches secret
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: |
          echo '{
            "cantcache.me": {
              "kind": "NixCache",
              "storeURI": "${{ secrets.CANT_CACHE_ME_NIX_S3_URI }}",
              "publicKeys": [
                "cantcache.me:Y+FHAKfx7S0pBkBMKpNMQtGKpILAfhmqUSnr5oNwNMs="
              ],
              "signingKeys": [
                "${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}"
              ]
            }
          }' \
            | ssh vm "sudo tee /var/lib/hercules-ci-agent/secrets/binary-caches.json"

      - name: Create the agent cluster join token secret
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: |
          echo "${{ secrets.HERCULES_CI_CLUSTER_JOIN_TOKEN }}" \
            | ssh vm "sudo tee /var/lib/hercules-ci-agent/secrets/cluster-join-token.key"

      - name: Fix permissions on the agent secrets directory
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: |
          ssh vm <<EOF
          sudo chown -R hercules-ci-agent /var/lib/hercules-ci-agent
          sudo chmod o-rwx /var/lib/hercules-ci-agent/secrets
          EOF

      - name: Start the agent
        env:
          ACTIVE: ${{ steps.hercules-ci-agent-active.outputs.active }}
        if: ${{ env.ACTIVE != 'true' }}
        run: ssh vm "sudo systemctl start hercules-ci-agent.service"

  terminate-nixos-azure-vm:
    name: Terminate NixOS Azure VM
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'terminate' }}
    needs: get-vhd-name
    outputs:
      vhd-name: ${{ needs.get-vhd-name.outputs.vhd-name }}
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_AUTH_MODE: login
      VHD_NAME: ${{ needs.get-vhd-name.outputs.vhd-name }}
    steps:
      - name: Log in to Azure
        uses: Azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      # TODO(@connorbaker): Allow multiple VMs to be created.
      - name: Delete matching VMs
        id: check-vm-running
        run: |
          VMS=$(az vm list --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}")
          echo "$VMS" | jq

          MATCHING_VMS=$(echo "$VMS" | jq -cr --arg name "${{ env.VHD_NAME }}" 'map(select(.name == $name))')
          echo "$MATCHING_VMS" | jq

          az vm delete \
              --ids $(echo "$MATCHING_VMS" | jq -cr '.[].id') \
              --yes \
            | jq -cr

      - name: Check if VHD exists in blob storage
        id: check-vhd-blob-exists
        run: |
          az storage blob exists \
              --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
              --name "${{ env.VHD_NAME }}.vhd" \
            | jq -cr '"vhd-blob-exists=\(.exists)"' \
            | tee -a "$GITHUB_OUTPUT"
