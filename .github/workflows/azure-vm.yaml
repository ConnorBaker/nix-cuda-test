name: Azure VM

defaults:
  run:
    shell: bash

on:
  workflow_dispatch:
    inputs:
      # TODO: Implement the ability to stop it
      # action:
      #   description: Action
      #   required: true
      #   type: choice
      #   options:
      #     - start
      #     - terminate
      size:
        description: Instance
        required: false
        default: Standard_HB120rs_v3
        type: choice
        # All numbers are for spot instances in East US as of 2023-06-03.
        # Standard_HB120rs_v3:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/hbv3-series
        # - 120 AMD EPYCâ„¢ 7V73X (Milan-X) CPU cores
        # - 448 GB of RAM
        # - 2 * 960 GB SSD
        # - $0.37/hour
        # Standard_NC8as_T4_v3:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/nct4-v3-series
        # - 1x NVIDIA Tesla T4 GPU with 16 GB of RAM
        # - 8 AMD EPYC 7V12 (Rome) CPU cores
        # - 56 GB of RAM
        # - 1 * 360 GB SSD
        # - $0.14/hour
        # Standard_NV36ads_A10_v5:
        # - https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series
        # - 1x NVIDIA A10 GPU with 24 GB of RAM
        # - 36 AMD EPYC 74F3V(Milan) CPU cores
        # - 440 GB of RAM
        # - 1 * 1440 GB SSD
        # - $1.28/hour
        options:
          - Standard_HB120rs_v3
          # - Standard_NC8as_T4_v3
          # - Standard_NV36ads_A10_v5


jobs:
  get-vhd-name:
    name: Create a reference to the VHD name for later use
    runs-on: ubuntu-latest
    outputs:
      vhd-name: ${{ steps.get-vhd-name.outputs.vhd-name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Nix
        uses: ./.github/actions/nix-setup
        with:
          cant-cache-me-nix-secret-signing-key: ${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}
          cloudflare-r2-access-key-id: ${{ secrets.CLOUDFLARE_R2_ACCESS_KEY_ID }}
          cloudflare-r2-secret-access-key: ${{ secrets.CLOUDFLARE_R2_SECRET_ACCESS_KEY }}

      - name: Get the VHD name
        id: get-vhd-name
        run: |
          nix eval --raw .#nixosConfigurations.azure.outPath \
            | sed 's|/nix/store/|vhd-name=|' \
            | tee -a "$GITHUB_OUTPUT"

  ensure-nixos-azure-vhd-blob-exists:
    name: Ensure NixOS Azure VHD blob exists
    runs-on: ubuntu-latest
    needs: get-vhd-name
    outputs:
      vhd-name: ${{ needs.get-vhd-name.outputs.vhd-name }}
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_AUTH_MODE: login
      VHD_NAME: ${{ needs.get-vhd-name.outputs.vhd-name }}
    steps:
      - name: Log in to Azure
        uses: Azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      - name: Check if VHD exists in blob storage
        id: check-vhd-blob-exists
        run: |
          az storage blob exists \
              --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
              --name "${{ env.VHD_NAME }}.vhd" \
            | jq -cr '"vhd-blob-exists=\(.exists)"' \
            | tee -a "$GITHUB_OUTPUT"

      - name: Restore the VHD from GitHub Actions' cache
        if: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists != 'true' }}
        id: restore-vhd
        uses: actions/cache/restore@v3
        with:
          key: ${{ env.VHD_NAME }}
          path: ./result/disk.vhd

      - name: Checkout
        if: ${{ steps.restore-vhd.outputs.cache-hit != 'true' }}
        uses: actions/checkout@v3

      - name: Set up Nix
        if: ${{ steps.restore-vhd.outputs.cache-hit != 'true' }}
        uses: ./.github/actions/nix-setup
        with:
          cant-cache-me-nix-secret-signing-key: ${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}
          cloudflare-r2-access-key-id: ${{ secrets.CLOUDFLARE_R2_ACCESS_KEY_ID }}
          cloudflare-r2-secret-access-key: ${{ secrets.CLOUDFLARE_R2_SECRET_ACCESS_KEY }}

      - name: Download the NAR for the VHD
        if: ${{ steps.restore-vhd.outputs.cache-hit != 'true' }}
        run: |
          nix store dump-path -v \
              --store https://cantcache.me \
              "/nix/store/${{ env.VHD_NAME }}" \
            > "./${{ env.VHD_NAME }}.nar"

      - name: Extract the VHD from the NAR
        if: ${{ steps.restore-vhd.outputs.cache-hit != 'true' }}
        run: |
          mkdir "./result"
          nix nar cat -v "./${{ env.VHD_NAME }}.nar" /disk.vhd \
            > "./result/disk.vhd"

      - name: Copy the VHD to GitHub Actions' cache
        if: ${{ steps.restore-vhd.outputs.cache-hit != 'true' }}
        uses: actions/cache/save@v3
        with:
          key: ${{ env.VHD_NAME }}
          path: ./result/disk.vhd

      - name: Upload the VHD to an Azure blob
        if: ${{ steps.check-vhd-blob-exists.outputs.vhd-blob-exists != 'true' }}
        run: |
          az storage blob upload \
            --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
            --file "./result/disk.vhd" \
            --max-connections 8 \
            --name "${{ env.VHD_NAME }}.vhd" \
            --type page

  # We use the store entry as the name of the VM.
  # This lets us check if a VM running on the same image already exists.
  ensure-nixos-azure-vm-is-running:
    name: Ensure NixOS Azure VM is running
    needs: ensure-nixos-azure-vhd-blob-exists
    runs-on: ubuntu-latest
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_AUTH_MODE: login
      VHD_NAME: ${{ needs.ensure-nixos-azure-vhd-blob-exists.outputs.vhd-name }}
    outputs:
      vhd-name: ${{ needs.ensure-nixos-azure-vhd-blob-exists.outputs.vhd-name }}
      public-ip-address: ${{ steps.get-public-ip-address.outputs.public-ip-address }}
    steps:
      - name: Log in to Azure
        uses: Azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      # TODO(@connorbaker): Allow multiple VMs to be created.
      - name: Check if VMs are running
        id: check-vm-running
        run: |
          VMS=$(az vm list --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}")
          echo "$VMS" | jq

          MATCHING_VMS=$(echo "$VMS" | jq -cr --arg name "${{ env.VHD_NAME }}" 'map(select(.name == $name))')
          echo "$MATCHING_VMS" | jq

          NUM_MATCHING_VMS=$(echo "$MATCHING_VMS" | jq -cr length)
          echo "Number of matching VMs: $NUM_MATCHING_VMS"
          case $NUM_MATCHING_VMS in
            0)
              echo "vm-running=false" >> "$GITHUB_OUTPUT"
              ;;
            1)
              echo "vm-running=true" >> "$GITHUB_OUTPUT"
              ;;
            *)
              echo "Multiple VMs with the same name found" >&2
              echo "$MATCHING_VMS" >&2
              exit 1
              ;;
          esac

      - name: Get the VHD blob URL
        id: get-vhd-blob-url
        if: ${{ steps.check-vm-running.outputs.vm-running != 'true' }}
        run: |
          az storage blob url \
              --container-name "${{ secrets.AZURE_STORAGE_CONTAINER }}" \
              --name "${{ env.VHD_NAME }}.vhd" \
            | jq -cr '"vhd-blob-url=\(.)"' \
            | tee -a "$GITHUB_OUTPUT"

      # TODO(@connorbaker): Allow multiple VMs to be created.
      - name: Create the VM
        if: ${{ steps.check-vm-running.outputs.vm-running != 'true' }}
        run: |
          az vm create \
              --attach-os-disk "${{ steps.get-vhd-blob-url.outputs.vhd-blob-url }}" \
              --ephemeral-os-disk true \
              --ephemeral-os-disk-placement CacheDisk \
              --eviction-policy Delete \
              --location eastus \
              --max-price 1.0 \
              --name "${{ env.VHD_NAME }}" \
              --nic-delete-option Delete \
              --os-disk-caching ReadOnly \
              --os-type linux \
              --priority Spot \
              --public-ip-address-allocation dynamic \
              --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}" \
              --size "${{ inputs.size }}" \
              --use-unmanaged-disk

      - name: Wait for the VM to be created
        if: ${{ steps.check-vm-running.outputs.vm-running != 'true' }}
        run: |
          az vm wait \
            --created \
            --name "${{ env.VHD_NAME }}" \
            --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}"

      - name: Get the public IP address of the VM
        id: get-public-ip-address
        run: |
          az vm show \
              --name "${{ env.VHD_NAME }}" \
              --resource-group "${{ secrets.AZURE_RESOURCE_GROUP }}" \
              --show-details \
            | jq -cr '"public-ip-address=\(.publicIps)"' \
            | tee -a "$GITHUB_OUTPUT"

  # TODO(@connorbaker): Refactor to allow setting up multiple VMs.
  ensure-nixos-azure-vm-hercules-ci-configured:
    name: Ensure NixOS Azure VM has Hercules CI configured
    runs-on: ubuntu-latest
    needs: ensure-nixos-azure-vm-is-running
    env:
      AZURE_VM_IP: ${{ needs.ensure-nixos-azure-vm-is-running.outputs.public-ip-address }}
    steps:
      - name: Set up the GitHub runner SSH keys
        run: |
          mkdir -p "$HOME/.ssh"
          chmod 700 "$HOME/.ssh"
          echo "${{ secrets.RUNNER_SSH_PRIVATE_KEY }}" > "$HOME/.ssh/ed25519"
          chmod 600 "$HOME/.ssh/ed25519"
          echo "${{ secrets.RUNNER_SSH_PUBLIC_KEY }}" > "$HOME/.ssh/ed25519.pub"
          chmod 600 "$HOME/.ssh/ed25519.pub"
          cat >> "$HOME/.ssh/config" <<EOF
          Host vm
            HostName ${{ env.AZURE_VM_IP }}
            User runner
            IdentityFile $HOME/.ssh/ed25519
            StrictHostKeyChecking no
          EOF
          chmod 600 "$HOME/.ssh/config"

      - name: Check if the agent is active (and so configured)
        id: hercules-ci-agent-active
        run: |
          ssh vm "sudo systemctl is-active --quiet hercules-ci-agent.service" \
            && echo "active=true" \
            || echo "active=false" \
            | tee -a "$GITHUB_OUTPUT"

      - name: Stop the agent
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: ssh vm "sudo systemctl stop hercules-ci-agent.service"

      - name: Create the agent secrets directory
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: ssh vm "sudo mkdir -p /var/lib/hercules-ci-agent/secrets"

      - name: Create the agent binary-caches secret
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: |
          echo '{
            "cantcache.me": {
              "kind": "NixCache",
              "storeURI": "${{ secrets.CANT_CACHE_ME_NIX_S3_URI }}",
              "publicKeys": [
                "cantcache.me:Y+FHAKfx7S0pBkBMKpNMQtGKpILAfhmqUSnr5oNwNMs="
              ],
              "signingKeys": [
                "${{ secrets.CANT_CACHE_ME_NIX_SECRET_SIGNING_KEY }}"
              ]
            }
          }' \
            | ssh vm "sudo tee /var/lib/hercules-ci-agent/secrets/binary-caches.json"

      - name: Create the agent cluster join token secret
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: |
          echo "${{ secrets.HERCULES_CI_CLUSTER_JOIN_TOKEN }}" \
            | ssh vm "sudo tee /var/lib/hercules-ci-agent/secrets/cluster-join-token.key"

      - name: Fix permissions on the agent secrets directory
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: |
          ssh vm <<EOF
          sudo chown -R hercules-ci-agent /var/lib/hercules-ci-agent
          sudo chmod o-rwx /var/lib/hercules-ci-agent/secrets
          EOF

      - name: Start the agent
        if: ${{ steps.hercules-ci-agent-active.outputs.active != 'true' }}
        run: ssh vm "sudo systemctl start hercules-ci-agent.service"
